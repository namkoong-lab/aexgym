{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch \n",
    "os.chdir(\"../..\")\n",
    "\n",
    "from aexgym.env import PersSyntheticEnv\n",
    "from aexgym.model import PersonalizedLinearModel\n",
    "from aexgym.agent import LinearTS, LinearUniform, LinearUCB, LinearRho\n",
    "from aexgym.objectives import contextual_best_arm, contextual_simple_regret\n",
    "from scripts.setup_script import make_uniform_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "n_days = 5\n",
    "n_arms = 10\n",
    "context_len = 5\n",
    "n_steps = n_days \n",
    "batch_size = 100\n",
    "s2 = 0.2 * torch.ones((n_days, 1))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#personalization \n",
    "\n",
    "#initialize parameterss\n",
    "n_objs = 1\n",
    "scaling = 1 / (batch_size*10)\n",
    "pers_beta, pers_sigma = make_uniform_prior(context_len*n_arms, scaling, n_objs=n_objs)\n",
    "context_mu, context_var = torch.ones(context_len), 1*torch.eye(context_len)\n",
    "#initialize synthetic and agent model \n",
    "model = PersonalizedLinearModel(\n",
    "    beta_0 = pers_beta, \n",
    "    sigma_0 = pers_sigma, \n",
    "    n_arms = n_arms, \n",
    "    s2 = s2,  \n",
    "    n_objs=n_objs,\n",
    "    standardize=True\n",
    ")\n",
    "\n",
    "#initialize synthetic environment\n",
    "env = PersSyntheticEnv(\n",
    "    model = model,  \n",
    "    context_mu = context_mu, \n",
    "    context_var = context_var, \n",
    "    context_len = context_len, \n",
    "    batch_size = batch_size, \n",
    "    n_steps = n_steps\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent \n",
    "agent = LinearUniform(model, \"Linear Uniform\")\n",
    "agent = LinearTS(model, \"Linear TS\", toptwo=False, n_samples = 100)\n",
    "#agent = LinearTS(model, \"Linear TS\", toptwo=True, n_samples = 100)\n",
    "agent = LinearRho(model, \"Linear Rho\", lr=0.1, weights= (0,1), cost_weights = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Regret:  0.008546624332666397\n",
      "Percent Arms Correct:  0.74\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m state_contexts, action_contexts, eval_contexts, costs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(contexts\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m contexts \u001b[38;5;129;01min\u001b[39;00m all_contexts)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#train agent \u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain_agent( \n\u001b[1;32m     27\u001b[0m     beta \u001b[38;5;241m=\u001b[39m beta, \n\u001b[1;32m     28\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m sigma, \n\u001b[1;32m     29\u001b[0m     cur_step \u001b[38;5;241m=\u001b[39m cur_step, \n\u001b[1;32m     30\u001b[0m     n_steps \u001b[38;5;241m=\u001b[39m n_steps, \n\u001b[1;32m     31\u001b[0m     train_context_sampler \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39msample_train_contexts, \n\u001b[1;32m     32\u001b[0m     eval_contexts \u001b[38;5;241m=\u001b[39m eval_contexts,\n\u001b[1;32m     33\u001b[0m     eval_action_contexts \u001b[38;5;241m=\u001b[39m action_contexts, \n\u001b[1;32m     34\u001b[0m     real_batch \u001b[38;5;241m=\u001b[39m batch_size, \n\u001b[1;32m     35\u001b[0m     print_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     36\u001b[0m     objective\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[1;32m     37\u001b[0m     costs\u001b[38;5;241m=\u001b[39mcosts,\n\u001b[1;32m     38\u001b[0m     repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m     39\u001b[0m )    \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#get probabilities\u001b[39;00m\n\u001b[1;32m     41\u001b[0m probs \u001b[38;5;241m=\u001b[39m agent(\n\u001b[1;32m     42\u001b[0m     beta \u001b[38;5;241m=\u001b[39m beta, \n\u001b[1;32m     43\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m sigma, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     objective \u001b[38;5;241m=\u001b[39m objective\n\u001b[1;32m     47\u001b[0m )\n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/agent/linear/linear_rho.py:88\u001b[0m, in \u001b[0;36mLinearRho.train_agent\u001b[0;34m(self, beta, sigma, cur_step, n_steps, train_context_sampler, eval_contexts, eval_action_contexts, real_batch, print_losses, objective, costs, repeats)\u001b[0m\n\u001b[1;32m     85\u001b[0m cov \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([get_cov(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, sigma[:, :, i], probs, train_context_list, cur_step, boost \u001b[38;5;241m=\u001b[39m boost, obj\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_objs)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)      \n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#calculate simple regret term \u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m simple_reg_loss \u001b[38;5;241m=\u001b[39m LinearQFn(beta, cov, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_zs, eval_features_all_arms, objective)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#calcuate cumulative regret term \u001b[39;00m\n\u001b[1;32m     91\u001b[0m train_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnkf,fd->nkd\u001b[39m\u001b[38;5;124m'\u001b[39m, train_features_all_arms, beta) \n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/agent/linear/agent_utils.py:56\u001b[0m, in \u001b[0;36mLinearQFn\u001b[0;34m(beta, cov, num_zs, features_all_arms, objective)\u001b[0m\n\u001b[1;32m     53\u001b[0m mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(mean, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#generate zs\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,size\u001b[38;5;241m=\u001b[39m(batch, k, num_zs, n_objs), device \u001b[38;5;241m=\u001b[39m device) \n\u001b[1;32m     57\u001b[0m var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnkc,cfd->nkfd\u001b[39m\u001b[38;5;124m'\u001b[39m, features_all_arms, cov)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#calculate variance\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_probs = False\n",
    "torch.manual_seed(0)\n",
    "objective = contextual_simple_regret()\n",
    "objective.weights = (0, 1)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "regret_list = []\n",
    "percent_arms_correct_list = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    env.reset()\n",
    "    #print(env.mean_matrix)\n",
    "    cumul_regret = 0\n",
    "    all_contexts, cur_step = env.reset()\n",
    "    beta, sigma = agent.model.reset()\n",
    "    #print(beta, sigma)\n",
    "    beta, sigma = beta.to(device), sigma.to(device)\n",
    "    \n",
    "    while env.n_steps - cur_step > 0:\n",
    "\n",
    "        #move to device \n",
    "        state_contexts, action_contexts, eval_contexts, costs = tuple(contexts.to(device) for contexts in all_contexts)\n",
    "\n",
    "        #train agent \n",
    "        agent.train_agent( \n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            cur_step = cur_step, \n",
    "            n_steps = n_steps, \n",
    "            train_context_sampler = env.sample_train_contexts, \n",
    "            eval_contexts = eval_contexts,\n",
    "            eval_action_contexts = action_contexts, \n",
    "            real_batch = batch_size, \n",
    "            print_losses=False, \n",
    "            objective=objective,\n",
    "            costs=costs,\n",
    "            repeats=10000\n",
    "        )    \n",
    "        #get probabilities\n",
    "        probs = agent(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            objective = objective\n",
    "        )\n",
    "     \n",
    "        #print probabilities \n",
    "        if print_probs == True:\n",
    "            print(agent.name, env.n_steps - cur_step, probs)\n",
    "        \n",
    "        #get actions and move to new state\n",
    "        actions = torch.distributions.Categorical(probs).sample()\n",
    "        #move to next environment state \n",
    "        all_contexts, sampled_rewards, sampled_features, cur_step  = env.step(\n",
    "            state_contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            actions = actions\n",
    "        )\n",
    "\n",
    "        rewards = objective(\n",
    "            agent_actions = actions,\n",
    "            true_rewards = env.get_true_rewards(state_contexts, action_contexts)\n",
    "        )\n",
    "\n",
    "        cumul_regret += rewards['regret']\n",
    "        \n",
    "        #update model state \n",
    "        beta, sigma = agent.model.update_posterior(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            rewards = sampled_rewards, \n",
    "            features = agent.model.feature_map(actions, state_contexts, action_contexts), \n",
    "            idx = cur_step-1\n",
    "        )\n",
    "\n",
    "    #get evaluation contexts and true rewards \n",
    "    eval_contexts = env.sample_eval_contexts(access=True).to(device)\n",
    "    true_eval_rewards = env.get_true_rewards(eval_contexts, action_contexts)\n",
    "    \n",
    "    fantasy_rewards = agent.fantasize(beta, eval_contexts, action_contexts).to(device)\n",
    "    agent_actions = torch.argmax(fantasy_rewards.squeeze(), dim=1)\n",
    "\n",
    "    #calculate results from objective \n",
    "    results_dict = objective(\n",
    "        agent_actions = agent_actions, \n",
    "        true_rewards = true_eval_rewards.to(device)\n",
    "    )\n",
    "\n",
    "    cumul_regret = cumul_regret / n_days\n",
    "    results_dict['regret'] = objective.weights[0] * cumul_regret + objective.weights[1] * results_dict['regret']\n",
    "    \n",
    "    #append results \n",
    "    percent_arms_correct_list.append(results_dict['percent_arms_correct'])\n",
    "    regret_list.append(results_dict['regret'])\n",
    "\n",
    "    #print results \n",
    "    if i % 10 == 0:\n",
    "        \n",
    "        print(i, \"Regret: \", np.mean(regret_list))\n",
    "        print(\"Percent Arms Correct: \", np.mean(percent_arms_correct_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jwenv-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

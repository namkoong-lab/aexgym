{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch \n",
    "os.chdir(\"../..\")\n",
    "\n",
    "from aexgym.env import ConstraintPersSyntheticEnv\n",
    "from aexgym.model import PersonalizedLinearModel\n",
    "from aexgym.agent import LinearTS, LinearUniform, LinearUCB, LinearRho\n",
    "from aexgym.objectives import contextual_best_arm, contextual_simple_regret\n",
    "from scripts.setup_script import make_uniform_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "n_days = 5\n",
    "n_arms = 10\n",
    "context_len = 5\n",
    "n_steps = n_days \n",
    "batch_size = 100\n",
    "s2 = 0.2 * torch.ones((n_days, 1))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "#personalization \n",
    "\n",
    "#initialize parameterss\n",
    "n_objs = 1\n",
    "scaling = 1 / (batch_size*10)\n",
    "pers_beta, pers_sigma = make_uniform_prior(context_len*n_arms, scaling, n_objs=n_objs)\n",
    "context_mu, context_var = torch.ones(context_len), 1*torch.eye(context_len)\n",
    "constraint_mu, constraint_var = torch.zeros(n_arms), 1*torch.eye(n_arms)\n",
    "print(pers_beta)\n",
    "pers_beta = 1*torch.ones_like(pers_beta)\n",
    "#initialize synthetic and agent model \n",
    "model = PersonalizedLinearModel(\n",
    "    beta_0 = pers_beta, \n",
    "    sigma_0 = pers_sigma, \n",
    "    n_arms = n_arms, \n",
    "    s2 = s2,  \n",
    "    n_objs=n_objs,\n",
    ")\n",
    "\n",
    "#initialize synthetic environment\n",
    "env = ConstraintPersSyntheticEnv(\n",
    "    model = model, \n",
    "    context_mu = context_mu, \n",
    "    context_var = context_var, \n",
    "    context_len = context_len, \n",
    "    batch_size = batch_size, \n",
    "    n_steps = n_steps,\n",
    "    constraint_mu = constraint_mu,\n",
    "    constraint_var = constraint_var\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent \n",
    "agent = LinearUniform(model, \"Linear Uniform\")\n",
    "#agent = LinearTS(model, \"Linear TS\", toptwo=False, n_samples = 100, constraint=True, cost_weight=0.01)\n",
    "#agent = LinearTS(model, \"Linear TS\", toptwo=True, n_samples = 100)\n",
    "#agent = LinearRho(model, \"Linear Rho\", lr=0.4, weights= (0,1), cost_weight = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Regret:  0.04553588479757309\n",
      "Percent Arms Correct:  0.38\n",
      "cost 3.5489953756332397\n",
      "10 Regret:  0.04459281494332985\n",
      "Percent Arms Correct:  0.48181818181818187\n",
      "cost 3.4576949693939905\n",
      "20 Regret:  0.048434684540899026\n",
      "Percent Arms Correct:  0.4723809523809523\n",
      "cost 3.4240952900477817\n",
      "30 Regret:  0.049373984411959684\n",
      "Percent Arms Correct:  0.44032258064516133\n",
      "cost 3.384240520577277\n",
      "40 Regret:  0.050387714819119474\n",
      "Percent Arms Correct:  0.43341463414634146\n",
      "cost 3.418499570794222\n",
      "50 Regret:  0.04974864623235429\n",
      "Percent Arms Correct:  0.41745098039215683\n",
      "cost 3.4343196083517635\n",
      "60 Regret:  0.04969234093565677\n",
      "Percent Arms Correct:  0.4186885245901639\n",
      "cost 3.450667718394858\n",
      "70 Regret:  0.04832845013982184\n",
      "Percent Arms Correct:  0.4191549295774648\n",
      "cost 3.4663860008750165\n",
      "80 Regret:  0.04917505866974408\n",
      "Percent Arms Correct:  0.4090123456790123\n",
      "cost 3.4499665594395297\n",
      "90 Regret:  0.049067592089688715\n",
      "Percent Arms Correct:  0.41021978021978023\n",
      "cost 3.4371942704195506\n",
      "100 Regret:  0.048267674065278016\n",
      "Percent Arms Correct:  0.410990099009901\n",
      "cost 3.43802880444149\n",
      "110 Regret:  0.04874911747856049\n",
      "Percent Arms Correct:  0.40684684684684697\n",
      "cost 3.4352934927016765\n",
      "120 Regret:  0.04887123265465306\n",
      "Percent Arms Correct:  0.40421487603305783\n",
      "cost 3.4292711635266455\n",
      "130 Regret:  0.04933327734683647\n",
      "Percent Arms Correct:  0.40312977099236635\n",
      "cost 3.4225131332419303\n",
      "140 Regret:  0.04924786735180739\n",
      "Percent Arms Correct:  0.4067375886524822\n",
      "cost 3.4067418148331607\n",
      "150 Regret:  0.0507392049397015\n",
      "Percent Arms Correct:  0.4047682119205298\n",
      "cost 3.4098934754235857\n",
      "160 Regret:  0.05144738169641506\n",
      "Percent Arms Correct:  0.39975155279503105\n",
      "cost 3.4034333140213295\n",
      "170 Regret:  0.05178745247543468\n",
      "Percent Arms Correct:  0.3918713450292397\n",
      "cost 3.4092604531879314\n",
      "180 Regret:  0.051628795866622945\n",
      "Percent Arms Correct:  0.3901657458563536\n",
      "cost 3.4135018079978985\n",
      "190 Regret:  0.051942539696352485\n",
      "Percent Arms Correct:  0.38643979057591626\n",
      "cost 3.4161433023620025\n",
      "200 Regret:  0.05202129202204483\n",
      "Percent Arms Correct:  0.38452736318407965\n",
      "cost 3.4024627726172927\n",
      "210 Regret:  0.0516796847386065\n",
      "Percent Arms Correct:  0.38559241706161146\n",
      "cost 3.4095305027837437\n",
      "220 Regret:  0.050971090509155895\n",
      "Percent Arms Correct:  0.3919457013574661\n",
      "cost 3.4099235461847814\n",
      "230 Regret:  0.051098229373639927\n",
      "Percent Arms Correct:  0.3911255411255411\n",
      "cost 3.4015964418004603\n",
      "240 Regret:  0.05098316764180776\n",
      "Percent Arms Correct:  0.3931120331950208\n",
      "cost 3.4046372228390944\n",
      "250 Regret:  0.05118712813099958\n",
      "Percent Arms Correct:  0.39187250996015943\n",
      "cost 3.4006020404190656\n",
      "260 Regret:  0.05102118471337632\n",
      "Percent Arms Correct:  0.39218390804597697\n",
      "cost 3.399334465635234\n",
      "270 Regret:  0.05064005017919789\n",
      "Percent Arms Correct:  0.39468634686346865\n",
      "cost 3.393864600750793\n",
      "280 Regret:  0.05048534901666843\n",
      "Percent Arms Correct:  0.39533807829181494\n",
      "cost 3.3920401833235583\n",
      "290 Regret:  0.05039913085516529\n",
      "Percent Arms Correct:  0.3947766323024055\n",
      "cost 3.3995006523795963\n",
      "300 Regret:  0.05046460931993823\n",
      "Percent Arms Correct:  0.39475083056478405\n",
      "cost 3.4042646335406954\n",
      "310 Regret:  0.0507292658295722\n",
      "Percent Arms Correct:  0.39340836012861735\n",
      "cost 3.4004352136823526\n",
      "320 Regret:  0.05063038398047497\n",
      "Percent Arms Correct:  0.39401869158878505\n",
      "cost 3.4021836695641374\n",
      "330 Regret:  0.05084715347829925\n",
      "Percent Arms Correct:  0.3936253776435046\n",
      "cost 3.4000240342854733\n",
      "340 Regret:  0.05114084388582784\n",
      "Percent Arms Correct:  0.39269794721407625\n",
      "cost 3.3990317011508773\n",
      "350 Regret:  0.05084114187752611\n",
      "Percent Arms Correct:  0.3944729344729344\n",
      "cost 3.3954562354631235\n",
      "360 Regret:  0.05105754161715363\n",
      "Percent Arms Correct:  0.39202216066482\n",
      "cost 3.3885012595937525\n",
      "370 Regret:  0.05066654290012373\n",
      "Percent Arms Correct:  0.3942857142857143\n",
      "cost 3.3884801929691086\n",
      "380 Regret:  0.050896445715193715\n",
      "Percent Arms Correct:  0.39073490813648293\n",
      "cost 3.390146840510406\n",
      "390 Regret:  0.051011149407676454\n",
      "Percent Arms Correct:  0.3887212276214834\n",
      "cost 3.391848050648599\n",
      "400 Regret:  0.05064668364304947\n",
      "Percent Arms Correct:  0.39132169576059855\n",
      "cost 3.3952283791621722\n",
      "410 Regret:  0.0509368583538463\n",
      "Percent Arms Correct:  0.3896593673965937\n",
      "cost 3.404161868739302\n",
      "420 Regret:  0.05109999422924129\n",
      "Percent Arms Correct:  0.388812351543943\n",
      "cost 3.4035353471575895\n",
      "430 Regret:  0.05133782013258929\n",
      "Percent Arms Correct:  0.38788863109048727\n",
      "cost 3.402367662318621\n",
      "440 Regret:  0.051160794907184046\n",
      "Percent Arms Correct:  0.3876643990929706\n",
      "cost 3.40467645886804\n",
      "450 Regret:  0.05112745658135219\n",
      "Percent Arms Correct:  0.38654101995565404\n",
      "cost 3.4075637002477626\n",
      "460 Regret:  0.050980151045578584\n",
      "Percent Arms Correct:  0.38876355748373104\n",
      "cost 3.409868774600246\n",
      "470 Regret:  0.05074944838460321\n",
      "Percent Arms Correct:  0.3905095541401274\n",
      "cost 3.408813849376265\n",
      "480 Regret:  0.050745356085917404\n",
      "Percent Arms Correct:  0.39083160083160084\n",
      "cost 3.4086956754791515\n",
      "490 Regret:  0.0507340573467686\n",
      "Percent Arms Correct:  0.3905091649694501\n",
      "cost 3.4048434038269058\n",
      "500 Regret:  0.05086701223859099\n",
      "Percent Arms Correct:  0.3902195608782435\n",
      "cost 3.4053930027518207\n",
      "510 Regret:  0.05091295588839938\n",
      "Percent Arms Correct:  0.3902739726027397\n",
      "cost 3.403239819984847\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mmean(costs[actions]) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(costs))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#move to next environment state \u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m all_contexts, sampled_rewards, sampled_features, cur_step  \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m     59\u001b[0m     state_contexts \u001b[38;5;241m=\u001b[39m state_contexts, \n\u001b[1;32m     60\u001b[0m     action_contexts \u001b[38;5;241m=\u001b[39m action_contexts, \n\u001b[1;32m     61\u001b[0m     actions \u001b[38;5;241m=\u001b[39m actions\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m rewards \u001b[38;5;241m=\u001b[39m objective(\n\u001b[1;32m     65\u001b[0m     agent_actions \u001b[38;5;241m=\u001b[39m actions,\n\u001b[1;32m     66\u001b[0m     true_rewards \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_true_rewards(state_contexts, action_contexts)\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     69\u001b[0m cumul_regret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregret\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/env/base_env.py:159\u001b[0m, in \u001b[0;36mBaseContextualEnv.step\u001b[0;34m(self, state_contexts, action_contexts, actions)\u001b[0m\n\u001b[1;32m    157\u001b[0m eval_contexts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_eval_contexts()\n\u001b[1;32m    158\u001b[0m action_contexts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_action_contexts()\n\u001b[0;32m--> 159\u001b[0m costs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_costs()\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m#get new state \u001b[39;00m\n\u001b[1;32m    162\u001b[0m all_contexts \u001b[38;5;241m=\u001b[39m state_contexts, action_contexts, eval_contexts, costs\n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/env/pers_env.py:44\u001b[0m, in \u001b[0;36mConstraintPersSampler.sample_costs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_costs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     43\u001b[0m     mvn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mMultivariateNormal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraint_mu, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraint_var)\n\u001b[0;32m---> 44\u001b[0m     costs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(mvn\u001b[38;5;241m.\u001b[39msample())\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m costs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/distributions/distribution.py:163\u001b[0m, in \u001b[0;36mDistribution.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample_shape: torch\u001b[38;5;241m.\u001b[39mSize \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize()) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Generates a sample_shape shaped sample or sample_shape shaped batch of\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    samples if the distribution parameters are batched.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrsample(sample_shape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/grad_mode.py:83\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''REMEMBER STANDARDIZE\n",
    "'''\n",
    "\n",
    "print_probs = False\n",
    "torch.manual_seed(0)\n",
    "objective = contextual_simple_regret()\n",
    "objective.weights = (0, 1)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "regret_list = []\n",
    "percent_arms_correct_list = []\n",
    "cost_list = []\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    env.reset()\n",
    "    cost = 0\n",
    "    #print(env.mean_matrix)\n",
    "    cumul_regret = 0\n",
    "    all_contexts, cur_step = env.reset()\n",
    "    beta, sigma = agent.model.reset()\n",
    "    #print(beta, sigma)\n",
    "    beta, sigma = beta.to(device), sigma.to(device)\n",
    "    \n",
    "    while env.n_steps - cur_step > 0:\n",
    "\n",
    "        #move to device \n",
    "        state_contexts, action_contexts, eval_contexts, costs = tuple(contexts.to(device) for contexts in all_contexts)\n",
    "        #train agent \n",
    "        agent.train_agent( \n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            cur_step = cur_step, \n",
    "            n_steps = n_steps, \n",
    "            train_context_sampler = env.sample_train_contexts, \n",
    "            eval_contexts = eval_contexts,\n",
    "            eval_action_contexts = action_contexts, \n",
    "            real_batch = batch_size, \n",
    "            print_losses=False, \n",
    "            objective=objective,\n",
    "            costs=costs,\n",
    "            repeats=10000\n",
    "        )    \n",
    "        #get probabilities\n",
    "        probs = agent(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            objective = objective,\n",
    "            costs = costs \n",
    "        )\n",
    "     \n",
    "        #print probabilities \n",
    "        if print_probs == True:\n",
    "            print(agent.name, env.n_steps - cur_step, probs)\n",
    "        \n",
    "        #get actions and move to new state\n",
    "        actions = torch.distributions.Categorical(probs).sample()\n",
    "        cost += (torch.mean(costs[actions]) - torch.min(costs)).item()\n",
    "        #move to next environment state \n",
    "        all_contexts, sampled_rewards, sampled_features, cur_step  = env.step(\n",
    "            state_contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            actions = actions\n",
    "        )\n",
    "\n",
    "        rewards = objective(\n",
    "            agent_actions = actions,\n",
    "            true_rewards = env.get_true_rewards(state_contexts, action_contexts)\n",
    "        )\n",
    "\n",
    "        cumul_regret += rewards['regret']\n",
    "        \n",
    "        #update model state \n",
    "        beta, sigma = agent.model.update_posterior(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            rewards = sampled_rewards, \n",
    "            features = agent.model.feature_map(actions, state_contexts, action_contexts), \n",
    "            idx = cur_step-1\n",
    "        )\n",
    "\n",
    "    #get evaluation contexts and true rewards \n",
    "    eval_contexts = env.sample_eval_contexts(access=True).to(device)\n",
    "    true_eval_rewards = env.get_true_rewards(eval_contexts, action_contexts)\n",
    "    \n",
    "    fantasy_rewards = agent.fantasize(beta, eval_contexts, action_contexts).to(device)\n",
    "    agent_actions = torch.argmax(fantasy_rewards.squeeze(), dim=1)\n",
    "\n",
    "    #calculate results from objective \n",
    "    results_dict = objective(\n",
    "        agent_actions = agent_actions, \n",
    "        true_rewards = true_eval_rewards.to(device)\n",
    "    )\n",
    "\n",
    "    cumul_regret = cumul_regret / n_days\n",
    "    results_dict['regret'] = objective.weights[0] * cumul_regret + objective.weights[1] * results_dict['regret']\n",
    "    \n",
    "    #append results \n",
    "    percent_arms_correct_list.append(results_dict['percent_arms_correct'])\n",
    "    regret_list.append(results_dict['regret'])\n",
    "    cost_list.append(cost)\n",
    "\n",
    "    #print results \n",
    "    if i % 10 == 0:\n",
    "        \n",
    "        print(i, \"Regret: \", np.mean(regret_list))\n",
    "        print(\"Percent Arms Correct: \", np.mean(percent_arms_correct_list))\n",
    "        print('cost', np.mean(cost_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

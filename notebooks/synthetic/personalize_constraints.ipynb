{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch \n",
    "os.chdir(\"../..\")\n",
    "\n",
    "from aexgym.env import ConstraintPersSyntheticEnv\n",
    "from aexgym.model import PersonalizedLinearModel\n",
    "from aexgym.agent import LinearTS, LinearUniform, LinearUCB, LinearRho\n",
    "from aexgym.objectives import contextual_best_arm, contextual_simple_regret\n",
    "from scripts.setup_script import make_uniform_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "n_days = 5\n",
    "n_arms = 10\n",
    "context_len = 5\n",
    "n_steps = n_days \n",
    "batch_size = 100\n",
    "s2 = 0.2 * torch.ones((n_days, 1))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "#personalization \n",
    "\n",
    "#initialize parameterss\n",
    "n_objs = 1\n",
    "scaling = 1 / (batch_size*10)\n",
    "pers_beta, pers_sigma = make_uniform_prior(context_len*n_arms, scaling, n_objs=n_objs)\n",
    "context_mu, context_var = torch.ones(context_len), 1*torch.eye(context_len)\n",
    "constraint_mu, constraint_var = torch.zeros(n_arms), 1*torch.eye(n_arms)\n",
    "print(pers_beta)\n",
    "pers_beta = 1*torch.ones_like(pers_beta)\n",
    "#initialize synthetic and agent model \n",
    "model = PersonalizedLinearModel(\n",
    "    beta_0 = pers_beta, \n",
    "    sigma_0 = pers_sigma, \n",
    "    n_arms = n_arms, \n",
    "    s2 = s2,  \n",
    "    n_objs=n_objs,\n",
    ")\n",
    "\n",
    "#initialize synthetic environment\n",
    "env = ConstraintPersSyntheticEnv(\n",
    "    model = model, \n",
    "    context_mu = context_mu, \n",
    "    context_var = context_var, \n",
    "    context_len = context_len, \n",
    "    batch_size = batch_size, \n",
    "    n_steps = n_steps,\n",
    "    constraint_mu = constraint_mu,\n",
    "    constraint_var = constraint_var\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent \n",
    "agent = LinearUniform(model, \"Linear Uniform\")\n",
    "agent = LinearTS(model, \"Linear TS\", toptwo=False, n_samples = 100, constraint=True, cost_weight=0.01)\n",
    "#agent = LinearTS(model, \"Linear TS\", toptwo=True, n_samples = 100)\n",
    "agent = LinearRho(model, \"Linear Rho\", lr=0.4, weights= (0,1), cost_weight = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Regret:  0.06704193353652954\n",
      "Percent Arms Correct:  0.23\n",
      "cost 0.8969033062458038\n",
      "10 Regret:  0.0396748503500765\n",
      "Percent Arms Correct:  0.5063636363636363\n",
      "cost 1.0197792986238545\n",
      "20 Regret:  0.042637006069223084\n",
      "Percent Arms Correct:  0.47\n",
      "cost 1.1040238180153428\n",
      "30 Regret:  0.041395819776000514\n",
      "Percent Arms Correct:  0.4574193548387097\n",
      "cost 1.0994701363266475\n",
      "40 Regret:  0.04677473549254057\n",
      "Percent Arms Correct:  0.4290243902439024\n",
      "cost 1.0819271629812515\n",
      "50 Regret:  0.04611596481545883\n",
      "Percent Arms Correct:  0.426078431372549\n",
      "cost 1.0720337754590255\n",
      "60 Regret:  0.04821145313135425\n",
      "Percent Arms Correct:  0.41836065573770487\n",
      "cost 1.0621151100599864\n",
      "70 Regret:  0.04817887918997399\n",
      "Percent Arms Correct:  0.4270422535211268\n",
      "cost 1.060985388549071\n",
      "80 Regret:  0.04762851733162447\n",
      "Percent Arms Correct:  0.42592592592592593\n",
      "cost 1.0633419780405584\n",
      "90 Regret:  0.04757276740293581\n",
      "Percent Arms Correct:  0.4308791208791209\n",
      "cost 1.0579678912206993\n",
      "100 Regret:  0.047546925336712655\n",
      "Percent Arms Correct:  0.4301980198019803\n",
      "cost 1.060773476497224\n",
      "110 Regret:  0.047348391419897475\n",
      "Percent Arms Correct:  0.43252252252252243\n",
      "cost 1.066582602205443\n",
      "120 Regret:  0.0476978349591767\n",
      "Percent Arms Correct:  0.42727272727272725\n",
      "cost 1.0626749279177632\n",
      "130 Regret:  0.04738875733375663\n",
      "Percent Arms Correct:  0.4318320610687023\n",
      "cost 1.065338473013453\n",
      "140 Regret:  0.04755205547780538\n",
      "Percent Arms Correct:  0.4302127659574469\n",
      "cost 1.0653173829039149\n",
      "150 Regret:  0.04718702116561745\n",
      "Percent Arms Correct:  0.43092715231788087\n",
      "cost 1.064361981369111\n",
      "160 Regret:  0.046785099186026344\n",
      "Percent Arms Correct:  0.4318633540372671\n",
      "cost 1.0626625033292156\n",
      "170 Regret:  0.046780116733555734\n",
      "Percent Arms Correct:  0.4336257309941521\n",
      "cost 1.0579660285525678\n",
      "180 Regret:  0.046473122924792665\n",
      "Percent Arms Correct:  0.43972375690607735\n",
      "cost 1.0598272097847574\n",
      "190 Regret:  0.046015138350212606\n",
      "Percent Arms Correct:  0.4419371727748691\n",
      "cost 1.0606312388115373\n",
      "200 Regret:  0.045854858023146934\n",
      "Percent Arms Correct:  0.4387562189054726\n",
      "cost 1.05926923790196\n",
      "210 Regret:  0.0461307784908791\n",
      "Percent Arms Correct:  0.43620853080568717\n",
      "cost 1.0580558300424385\n",
      "220 Regret:  0.0459807530589132\n",
      "Percent Arms Correct:  0.43552036199095023\n",
      "cost 1.0601338807796865\n",
      "230 Regret:  0.04617666379141407\n",
      "Percent Arms Correct:  0.4338528138528138\n",
      "cost 1.0626161163229317\n",
      "240 Regret:  0.04594510660170582\n",
      "Percent Arms Correct:  0.436804979253112\n",
      "cost 1.0628387214870991\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m state_contexts, action_contexts, eval_contexts, costs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(contexts\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m contexts \u001b[38;5;129;01min\u001b[39;00m all_contexts)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#train agent \u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain_agent( \n\u001b[1;32m     30\u001b[0m     beta \u001b[38;5;241m=\u001b[39m beta, \n\u001b[1;32m     31\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m sigma, \n\u001b[1;32m     32\u001b[0m     cur_step \u001b[38;5;241m=\u001b[39m cur_step, \n\u001b[1;32m     33\u001b[0m     n_steps \u001b[38;5;241m=\u001b[39m n_steps, \n\u001b[1;32m     34\u001b[0m     train_context_sampler \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39msample_train_contexts, \n\u001b[1;32m     35\u001b[0m     eval_contexts \u001b[38;5;241m=\u001b[39m eval_contexts,\n\u001b[1;32m     36\u001b[0m     eval_action_contexts \u001b[38;5;241m=\u001b[39m action_contexts, \n\u001b[1;32m     37\u001b[0m     real_batch \u001b[38;5;241m=\u001b[39m batch_size, \n\u001b[1;32m     38\u001b[0m     print_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     39\u001b[0m     objective\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[1;32m     40\u001b[0m     costs\u001b[38;5;241m=\u001b[39mcosts,\n\u001b[1;32m     41\u001b[0m     budget\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m,\n\u001b[1;32m     42\u001b[0m     repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m     43\u001b[0m )    \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#get probabilities\u001b[39;00m\n\u001b[1;32m     45\u001b[0m probs \u001b[38;5;241m=\u001b[39m agent(\n\u001b[1;32m     46\u001b[0m     beta \u001b[38;5;241m=\u001b[39m beta, \n\u001b[1;32m     47\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m sigma, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     costs \u001b[38;5;241m=\u001b[39m costs \n\u001b[1;32m     52\u001b[0m )\n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/agent/linear/linear_rho.py:89\u001b[0m, in \u001b[0;36mLinearRho.train_agent\u001b[0;34m(self, beta, sigma, cur_step, n_steps, train_context_sampler, eval_contexts, eval_action_contexts, real_batch, print_losses, objective, costs, budget, repeats)\u001b[0m\n\u001b[1;32m     86\u001b[0m cov \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([get_cov(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, sigma[:, :, i], probs, train_context_list, cur_step, boost \u001b[38;5;241m=\u001b[39m boost, obj\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_objs)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)      \n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#calculate simple regret term \u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m simple_reg_loss \u001b[38;5;241m=\u001b[39m LinearQFn(beta, cov, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_zs, eval_features_all_arms, objective)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#calcuate cumulative regret term \u001b[39;00m\n\u001b[1;32m     92\u001b[0m train_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnkf,fd->nkd\u001b[39m\u001b[38;5;124m'\u001b[39m, train_features_all_arms, beta) \n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/agent/linear/agent_utils.py:56\u001b[0m, in \u001b[0;36mLinearQFn\u001b[0;34m(beta, cov, num_zs, features_all_arms, objective)\u001b[0m\n\u001b[1;32m     53\u001b[0m mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(mean, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#generate zs\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,size\u001b[38;5;241m=\u001b[39m(batch, k, num_zs, n_objs), device \u001b[38;5;241m=\u001b[39m device) \n\u001b[1;32m     57\u001b[0m var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnkc,cfd->nkfd\u001b[39m\u001b[38;5;124m'\u001b[39m, features_all_arms, cov)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#calculate variance\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''REMEMBER STANDARDIZE\n",
    "'''\n",
    "\n",
    "print_probs = False\n",
    "torch.manual_seed(0)\n",
    "objective = contextual_simple_regret()\n",
    "objective.weights = (0, 1)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "regret_list = []\n",
    "percent_arms_correct_list = []\n",
    "cost_list = []\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    env.reset()\n",
    "    cost = 0\n",
    "    #print(env.mean_matrix)\n",
    "    cumul_regret = 0\n",
    "    all_contexts, cur_step = env.reset()\n",
    "    beta, sigma = agent.model.reset()\n",
    "    #print(beta, sigma)\n",
    "    beta, sigma = beta.to(device), sigma.to(device)\n",
    "    \n",
    "    while env.n_steps - cur_step > 0:\n",
    "\n",
    "        #move to device \n",
    "        state_contexts, action_contexts, eval_contexts, costs = tuple(contexts.to(device) for contexts in all_contexts)\n",
    "        #train agent \n",
    "        agent.train_agent( \n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            cur_step = cur_step, \n",
    "            n_steps = n_steps, \n",
    "            train_context_sampler = env.sample_train_contexts, \n",
    "            eval_contexts = eval_contexts,\n",
    "            eval_action_contexts = action_contexts, \n",
    "            real_batch = batch_size, \n",
    "            print_losses=False, \n",
    "            objective=objective,\n",
    "            costs=costs,\n",
    "            budget=0.23,\n",
    "            repeats=10000\n",
    "        )    \n",
    "        #get probabilities\n",
    "        probs = agent(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            objective = objective,\n",
    "            costs = costs \n",
    "        )\n",
    "     \n",
    "        #print probabilities \n",
    "        if print_probs == True:\n",
    "            print(agent.name, env.n_steps - cur_step, probs)\n",
    "        \n",
    "        #get actions and move to new state\n",
    "        actions = torch.distributions.Categorical(probs).sample()\n",
    "        cost += (torch.mean(costs[actions]) - torch.min(costs)).item()\n",
    "        #move to next environment state \n",
    "        all_contexts, sampled_rewards, sampled_features, cur_step  = env.step(\n",
    "            state_contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            actions = actions\n",
    "        )\n",
    "\n",
    "        rewards = objective(\n",
    "            agent_actions = actions,\n",
    "            true_rewards = env.get_true_rewards(state_contexts, action_contexts)\n",
    "        )\n",
    "\n",
    "        cumul_regret += rewards['regret']\n",
    "        \n",
    "        #update model state \n",
    "        beta, sigma = agent.model.update_posterior(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            rewards = sampled_rewards, \n",
    "            features = agent.model.feature_map(actions, state_contexts, action_contexts), \n",
    "            idx = cur_step-1\n",
    "        )\n",
    "\n",
    "    #get evaluation contexts and true rewards \n",
    "    eval_contexts = env.sample_eval_contexts(access=True).to(device)\n",
    "    true_eval_rewards = env.get_true_rewards(eval_contexts, action_contexts)\n",
    "    \n",
    "    fantasy_rewards = agent.fantasize(beta, eval_contexts, action_contexts).to(device)\n",
    "    agent_actions = torch.argmax(fantasy_rewards.squeeze(), dim=1)\n",
    "\n",
    "    #calculate results from objective \n",
    "    results_dict = objective(\n",
    "        agent_actions = agent_actions, \n",
    "        true_rewards = true_eval_rewards.to(device)\n",
    "    )\n",
    "\n",
    "    cumul_regret = cumul_regret / n_days\n",
    "    results_dict['regret'] = objective.weights[0] * cumul_regret + objective.weights[1] * results_dict['regret']\n",
    "    \n",
    "    #append results \n",
    "    percent_arms_correct_list.append(results_dict['percent_arms_correct'])\n",
    "    regret_list.append(results_dict['regret'])\n",
    "    cost_list.append(cost)\n",
    "\n",
    "    #print results \n",
    "    if i % 10 == 0:\n",
    "        \n",
    "        print(i, \"Regret: \", np.mean(regret_list))\n",
    "        print(\"Percent Arms Correct: \", np.mean(percent_arms_correct_list))\n",
    "        print('cost', np.mean(cost_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

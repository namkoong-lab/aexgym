{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch \n",
    "os.chdir(\"../..\")\n",
    "\n",
    "from aexgym.env import ConstraintPersSyntheticEnv\n",
    "from aexgym.model import PersonalizedLinearModel\n",
    "from aexgym.agent import LinearTS, LinearUniform, LinearUCB, LinearRho\n",
    "from aexgym.objectives import contextual_best_arm, contextual_simple_regret\n",
    "from scripts.setup_script import make_uniform_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "n_days = 5\n",
    "n_arms = 10\n",
    "context_len = 5\n",
    "n_steps = n_days \n",
    "batch_size = 100\n",
    "s2 = 0.2 * torch.ones((n_days, 1))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "#personalization \n",
    "\n",
    "#initialize parameterss\n",
    "n_objs = 1\n",
    "scaling = 1 / (batch_size*10)\n",
    "pers_beta, pers_sigma = make_uniform_prior(context_len*n_arms, scaling, n_objs=n_objs)\n",
    "context_mu, context_var = torch.ones(context_len), 1*torch.eye(context_len)\n",
    "constraint_mu, constraint_var = torch.zeros(n_arms), 1*torch.eye(n_arms)\n",
    "print(pers_beta)\n",
    "pers_beta = 1*torch.ones_like(pers_beta)\n",
    "#initialize synthetic and agent model \n",
    "model = PersonalizedLinearModel(\n",
    "    beta_0 = pers_beta, \n",
    "    sigma_0 = pers_sigma, \n",
    "    n_arms = n_arms, \n",
    "    s2 = s2,  \n",
    "    n_objs=n_objs,\n",
    ")\n",
    "\n",
    "#initialize synthetic environment\n",
    "env = ConstraintPersSyntheticEnv(\n",
    "    model = model, \n",
    "    context_mu = context_mu, \n",
    "    context_var = context_var, \n",
    "    context_len = context_len, \n",
    "    batch_size = batch_size, \n",
    "    n_steps = n_steps,\n",
    "    constraint_mu = constraint_mu,\n",
    "    constraint_var = constraint_var\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent \n",
    "agent = LinearUniform(model, \"Linear Uniform\")\n",
    "agent = LinearTS(model, \"Linear TS\", toptwo=False, n_samples = 100, constraint=True, cost_weight=0.01)\n",
    "#agent = LinearTS(model, \"Linear TS\", toptwo=True, n_samples = 100)\n",
    "agent = LinearRho(model, \"Linear Rho\", lr=0.4, weights= (0,1), cost_weight = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Regret:  0.0574474111199379\n",
      "Percent Arms Correct:  0.23\n",
      "cost 0.8574609160423279\n",
      "10 Regret:  0.046201051636175675\n",
      "Percent Arms Correct:  0.44999999999999996\n",
      "cost 0.9616643705151298\n",
      "20 Regret:  0.05000010081788614\n",
      "Percent Arms Correct:  0.4166666666666667\n",
      "cost 0.982538790220306\n",
      "30 Regret:  0.048315490280548413\n",
      "Percent Arms Correct:  0.4083870967741935\n",
      "cost 0.992630997492421\n",
      "40 Regret:  0.047956499532319424\n",
      "Percent Arms Correct:  0.40487804878048783\n",
      "cost 0.9819442102821861\n",
      "50 Regret:  0.046906108684910866\n",
      "Percent Arms Correct:  0.4082352941176471\n",
      "cost 0.979341349911456\n",
      "60 Regret:  0.04876951535125492\n",
      "Percent Arms Correct:  0.4111475409836065\n",
      "cost 0.9766663413067334\n",
      "70 Regret:  0.04794452729648058\n",
      "Percent Arms Correct:  0.422112676056338\n",
      "cost 0.9824182904836997\n",
      "80 Regret:  0.04817466099206128\n",
      "Percent Arms Correct:  0.41654320987654325\n",
      "cost 0.9836991627551155\n",
      "90 Regret:  0.04806911779035415\n",
      "Percent Arms Correct:  0.41956043956043965\n",
      "cost 0.9837274825261845\n",
      "100 Regret:  0.04731017076825299\n",
      "Percent Arms Correct:  0.42405940594059405\n",
      "cost 0.9862388300497343\n",
      "110 Regret:  0.04679160288677694\n",
      "Percent Arms Correct:  0.42747747747747744\n",
      "cost 0.9844567375341514\n",
      "120 Regret:  0.04700642759904019\n",
      "Percent Arms Correct:  0.4228099173553719\n",
      "cost 0.9809223090506289\n",
      "130 Regret:  0.04670080581310255\n",
      "Percent Arms Correct:  0.4260305343511451\n",
      "cost 0.9800366402411734\n",
      "140 Regret:  0.04702625410789822\n",
      "Percent Arms Correct:  0.42404255319148937\n",
      "cost 0.9809879732734346\n",
      "150 Regret:  0.04735301208330799\n",
      "Percent Arms Correct:  0.42278145695364244\n",
      "cost 0.9753548839185017\n",
      "160 Regret:  0.046408621226986926\n",
      "Percent Arms Correct:  0.4272670807453416\n",
      "cost 0.9761447757251144\n",
      "170 Regret:  0.04655451072946365\n",
      "Percent Arms Correct:  0.4266666666666667\n",
      "cost 0.9742961089992732\n",
      "180 Regret:  0.0473676736579786\n",
      "Percent Arms Correct:  0.4238674033149171\n",
      "cost 0.9739657802435245\n",
      "190 Regret:  0.047147687644019956\n",
      "Percent Arms Correct:  0.42476439790575915\n",
      "cost 0.9749132364169153\n",
      "200 Regret:  0.04737379627339357\n",
      "Percent Arms Correct:  0.4207960199004976\n",
      "cost 0.9731018812909945\n",
      "210 Regret:  0.04726187584003637\n",
      "Percent Arms Correct:  0.42056872037914694\n",
      "cost 0.9766027240426902\n",
      "220 Regret:  0.04734817233433888\n",
      "Percent Arms Correct:  0.4184162895927602\n",
      "cost 0.9747681675891801\n",
      "230 Regret:  0.04720713982148927\n",
      "Percent Arms Correct:  0.41796536796536804\n",
      "cost 0.9757699747831919\n",
      "240 Regret:  0.04701819522229035\n",
      "Percent Arms Correct:  0.4199585062240664\n",
      "cost 0.9744970155532058\n",
      "250 Regret:  0.04671081414342342\n",
      "Percent Arms Correct:  0.42266932270916335\n",
      "cost 0.9728121479743149\n",
      "260 Regret:  0.046538552353194276\n",
      "Percent Arms Correct:  0.42157088122605363\n",
      "cost 0.9727516488967841\n",
      "270 Regret:  0.04625781428934903\n",
      "Percent Arms Correct:  0.4249077490774908\n",
      "cost 0.9722907940523761\n",
      "280 Regret:  0.04619027599294627\n",
      "Percent Arms Correct:  0.424270462633452\n",
      "cost 0.9731900119868993\n",
      "290 Regret:  0.046068182061474346\n",
      "Percent Arms Correct:  0.42278350515463925\n",
      "cost 0.9742780225959533\n",
      "300 Regret:  0.0458404316777283\n",
      "Percent Arms Correct:  0.42355481727574756\n",
      "cost 0.9757356917751687\n",
      "310 Regret:  0.04582574598077936\n",
      "Percent Arms Correct:  0.42266881028938913\n",
      "cost 0.9757847266864642\n",
      "320 Regret:  0.04560112112819731\n",
      "Percent Arms Correct:  0.42364485981308414\n",
      "cost 0.9766509366334877\n",
      "330 Regret:  0.04532620709057436\n",
      "Percent Arms Correct:  0.4262235649546827\n",
      "cost 0.978995753317126\n",
      "340 Regret:  0.04524047133209637\n",
      "Percent Arms Correct:  0.42560117302052786\n",
      "cost 0.9785670978003496\n",
      "350 Regret:  0.04515241994058177\n",
      "Percent Arms Correct:  0.42572649572649574\n",
      "cost 0.9790694501696744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m state_contexts, action_contexts, eval_contexts, costs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(contexts\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m contexts \u001b[38;5;129;01min\u001b[39;00m all_contexts)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#train agent \u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain_agent( \n\u001b[1;32m     30\u001b[0m     beta \u001b[38;5;241m=\u001b[39m beta, \n\u001b[1;32m     31\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m sigma, \n\u001b[1;32m     32\u001b[0m     cur_step \u001b[38;5;241m=\u001b[39m cur_step, \n\u001b[1;32m     33\u001b[0m     n_steps \u001b[38;5;241m=\u001b[39m n_steps, \n\u001b[1;32m     34\u001b[0m     train_context_sampler \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39msample_train_contexts, \n\u001b[1;32m     35\u001b[0m     eval_contexts \u001b[38;5;241m=\u001b[39m eval_contexts,\n\u001b[1;32m     36\u001b[0m     eval_action_contexts \u001b[38;5;241m=\u001b[39m action_contexts, \n\u001b[1;32m     37\u001b[0m     real_batch \u001b[38;5;241m=\u001b[39m batch_size, \n\u001b[1;32m     38\u001b[0m     print_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     39\u001b[0m     objective\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[1;32m     40\u001b[0m     costs\u001b[38;5;241m=\u001b[39mcosts,\n\u001b[1;32m     41\u001b[0m     budget\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.23\u001b[39m,\n\u001b[1;32m     42\u001b[0m     repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m     43\u001b[0m )    \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#get probabilities\u001b[39;00m\n\u001b[1;32m     45\u001b[0m probs \u001b[38;5;241m=\u001b[39m agent(\n\u001b[1;32m     46\u001b[0m     beta \u001b[38;5;241m=\u001b[39m beta, \n\u001b[1;32m     47\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m sigma, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     costs \u001b[38;5;241m=\u001b[39m costs \n\u001b[1;32m     52\u001b[0m )\n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/agent/linear/linear_rho.py:89\u001b[0m, in \u001b[0;36mLinearRho.train_agent\u001b[0;34m(self, beta, sigma, cur_step, n_steps, train_context_sampler, eval_contexts, eval_action_contexts, real_batch, print_losses, objective, costs, budget, repeats)\u001b[0m\n\u001b[1;32m     86\u001b[0m cov \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([get_cov(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, sigma[:, :, i], probs, train_context_list, cur_step, boost \u001b[38;5;241m=\u001b[39m boost, obj\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_objs)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)      \n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#calculate simple regret term \u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m simple_reg_loss \u001b[38;5;241m=\u001b[39m LinearQFn(beta, cov, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_zs, eval_features_all_arms, objective)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#calcuate cumulative regret term \u001b[39;00m\n\u001b[1;32m     92\u001b[0m train_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnkf,fd->nkd\u001b[39m\u001b[38;5;124m'\u001b[39m, train_features_all_arms, beta) \n",
      "File \u001b[0;32m~/repos/aexgym/aexgym/agent/linear/agent_utils.py:56\u001b[0m, in \u001b[0;36mLinearQFn\u001b[0;34m(beta, cov, num_zs, features_all_arms, objective)\u001b[0m\n\u001b[1;32m     53\u001b[0m mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(mean, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#generate zs\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,size\u001b[38;5;241m=\u001b[39m(batch, k, num_zs, n_objs), device \u001b[38;5;241m=\u001b[39m device) \n\u001b[1;32m     57\u001b[0m var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnkc,cfd->nkfd\u001b[39m\u001b[38;5;124m'\u001b[39m, features_all_arms, cov)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#calculate variance\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''REMEMBER STANDARDIZE\n",
    "'''\n",
    "\n",
    "print_probs = False\n",
    "torch.manual_seed(0)\n",
    "objective = contextual_simple_regret()\n",
    "objective.weights = (0, 1)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "regret_list = []\n",
    "percent_arms_correct_list = []\n",
    "cost_list = []\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    env.reset()\n",
    "    cost = 0\n",
    "    #print(env.mean_matrix)\n",
    "    cumul_regret = 0\n",
    "    all_contexts, cur_step = env.reset()\n",
    "    beta, sigma = agent.model.reset()\n",
    "    #print(beta, sigma)\n",
    "    beta, sigma = beta.to(device), sigma.to(device)\n",
    "    \n",
    "    while env.n_steps - cur_step > 0:\n",
    "\n",
    "        #move to device \n",
    "        state_contexts, action_contexts, eval_contexts, costs = tuple(contexts.to(device) for contexts in all_contexts)\n",
    "        #train agent \n",
    "        agent.train_agent( \n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            cur_step = cur_step, \n",
    "            n_steps = n_steps, \n",
    "            train_context_sampler = env.sample_train_contexts, \n",
    "            eval_contexts = eval_contexts,\n",
    "            eval_action_contexts = action_contexts, \n",
    "            real_batch = batch_size, \n",
    "            print_losses=False, \n",
    "            objective=objective,\n",
    "            costs=costs,\n",
    "            budget=0.23,\n",
    "            repeats=10000\n",
    "        )    \n",
    "        #get probabilities\n",
    "        probs = agent(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            objective = objective,\n",
    "            costs = costs \n",
    "        )\n",
    "     \n",
    "        #print probabilities \n",
    "        if print_probs == True:\n",
    "            print(agent.name, env.n_steps - cur_step, probs)\n",
    "        \n",
    "        #get actions and move to new state\n",
    "        actions = torch.distributions.Categorical(probs).sample()\n",
    "        cost += (torch.mean(costs[actions]) - torch.min(costs)).item()\n",
    "        #move to next environment state \n",
    "        all_contexts, sampled_rewards, sampled_features, cur_step  = env.step(\n",
    "            state_contexts = state_contexts, \n",
    "            action_contexts = action_contexts, \n",
    "            actions = actions\n",
    "        )\n",
    "\n",
    "        rewards = objective(\n",
    "            agent_actions = actions,\n",
    "            true_rewards = env.get_true_rewards(state_contexts, action_contexts)\n",
    "        )\n",
    "\n",
    "        cumul_regret += rewards['regret']\n",
    "        \n",
    "        #update model state \n",
    "        beta, sigma = agent.model.update_posterior(\n",
    "            beta = beta, \n",
    "            sigma = sigma, \n",
    "            rewards = sampled_rewards, \n",
    "            features = agent.model.feature_map(actions, state_contexts, action_contexts), \n",
    "            idx = cur_step-1\n",
    "        )\n",
    "\n",
    "    #get evaluation contexts and true rewards \n",
    "    eval_contexts = env.sample_eval_contexts(access=True).to(device)\n",
    "    true_eval_rewards = env.get_true_rewards(eval_contexts, action_contexts)\n",
    "    \n",
    "    fantasy_rewards = agent.fantasize(beta, eval_contexts, action_contexts).to(device)\n",
    "    agent_actions = torch.argmax(fantasy_rewards.squeeze(), dim=1)\n",
    "\n",
    "    #calculate results from objective \n",
    "    results_dict = objective(\n",
    "        agent_actions = agent_actions, \n",
    "        true_rewards = true_eval_rewards.to(device)\n",
    "    )\n",
    "\n",
    "    cumul_regret = cumul_regret / n_days\n",
    "    results_dict['regret'] = objective.weights[0] * cumul_regret + objective.weights[1] * results_dict['regret']\n",
    "    \n",
    "    #append results \n",
    "    percent_arms_correct_list.append(results_dict['percent_arms_correct'])\n",
    "    regret_list.append(results_dict['regret'])\n",
    "    cost_list.append(cost)\n",
    "\n",
    "    #print results \n",
    "    if i % 10 == 0:\n",
    "        \n",
    "        print(i, \"Regret: \", np.mean(regret_list))\n",
    "        print(\"Percent Arms Correct: \", np.mean(percent_arms_correct_list))\n",
    "        print('cost', np.mean(cost_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
